# ğŸ”§ Configuration API Ollama - RÃ‰SOLU

## âœ… Configuration correcte

Votre API Ollama fonctionne via un proxy/gateway. Voici la configuration correcte :

### Variables d'environnement

```bash
# URL complÃ¨te avec /api/chat
VITE_CHATBOT_API_URL=https://ai.jobsacademie.tech/api/chat

# ModÃ¨le exact : gemma:2b (PAS gemma2:2b)
VITE_OLLAMA_MODEL=gemma:2b
```

## ğŸ“¡ Format de l'API

### RequÃªte

```json
{
  "model": "gemma:2b",
  "messages": [
    {
      "role": "user",
      "content": "Votre message ici"
    }
  ],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 500
  }
}
```

### RÃ©ponse

```json
{
  "model": "gemma:2b",
  "created_at": "2025-12-03T...",
  "message": {
    "role": "assistant",
    "content": "RÃ©ponse de Gemma ici"
  },
  "done": true,
  "done_reason": "stop",
  "total_duration": 1923507084,
  "prompt_eval_count": 28,
  "eval_count": 18
}
```

## ğŸ§ª Tests effectuÃ©s

âœ… Liste des modÃ¨les : `/api/tags` â†’ `gemma:2b` disponible
âœ… Message simple : Fonctionne en espagnol
âœ… Questions complexes : RÃ©ponses pertinentes
âœ… Contexte de conversation : GÃ¨re l'historique
âœ… Temps de rÃ©ponse : ~3 secondes (acceptable)

## ğŸ”„ Modifications apportÃ©es

### 1. Service Gemma (`src/services/gemmaService.js`)

**Avant** :
```javascript
const payload = {
  message: message,
  model: this.model,
  sessionId: this.sessionId
};
```

**AprÃ¨s** :
```javascript
const payload = {
  model: this.model,
  messages: [
    {
      role: 'user',
      content: message
    }
  ],
  stream: false,
  options: {
    temperature: 0.7,
    num_predict: this.maxTokens
  }
};
```

### 2. Validation de rÃ©ponse

**Avant** :
```javascript
const message = response.message || response.response || response.text;
```

**AprÃ¨s** :
```javascript
// GÃ¨re le format Ollama : response.message.content
if (response.message && response.message.content) {
  message = response.message.content;
}
```

### 3. Gestion du contexte

Le service envoie maintenant l'historique des messages au format Ollama :

```javascript
messages: [
  { role: 'user', content: 'Message 1' },
  { role: 'assistant', content: 'RÃ©ponse 1' },
  { role: 'user', content: 'Message 2' }
]
```

## ğŸš€ Pour tester

### 1. RedÃ©marrez le serveur de dÃ©veloppement

```bash
# ArrÃªtez le serveur actuel (Ctrl+C)
# Puis relancez
npm run dev
```

### 2. Testez dans l'interface

1. Ouvrez http://localhost:5173
2. Cliquez sur le chatbot (icÃ´ne en bas Ã  droite)
3. Tapez : "What is the capital of Roma?"
4. Vous devriez obtenir une rÃ©ponse de Gemma !

### 3. Tests en ligne de commande

```bash
# Test rapide
./test-api.sh

# Test manuel
curl -X POST https://ai.jobsacademie.tech/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma:2b",
    "messages": [{"role": "user", "content": "Hola"}],
    "stream": false
  }'
```

### 4. Test dans la console du navigateur

```javascript
// Ouvrez la console (F12)
await gemmaService.sendMessage('What is the capital of Roma?');

// Devrait retourner
{
  message: "La capital de Roma es Roma. Es la ciudad mÃ¡s grande...",
  fromCache: false,
  sessionId: "...",
  timestamp: ...
}
```

## ğŸ” DÃ©bogage

Si le chatbot ne fonctionne toujours pas :

### 1. VÃ©rifiez les variables d'environnement

```bash
# Dans le terminal
echo $VITE_CHATBOT_API_URL
# Devrait afficher: https://ai.jobsacademie.tech/api/chat

echo $VITE_OLLAMA_MODEL
# Devrait afficher: gemma:2b
```

### 2. VÃ©rifiez la console du navigateur (F12)

Regardez les erreurs dans :
- Console
- Network tab (onglet RÃ©seau)

### 3. Testez directement l'API

```bash
curl -X POST https://ai.jobsacademie.tech/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma:2b",
    "messages": [{"role": "user", "content": "test"}],
    "stream": false
  }' | jq '.message.content'
```

## ğŸ“Š Performance

- **Temps de rÃ©ponse moyen** : 3 secondes
- **DisponibilitÃ©** : âœ… En ligne
- **Latence** : Acceptable pour un chatbot
- **QualitÃ© des rÃ©ponses** : Bonne (en espagnol)

## ğŸ¯ Prochaines optimisations possibles

1. **Cache cÃ´tÃ© serveur** : RÃ©duire la latence
2. **Streaming** : Afficher la rÃ©ponse au fur et Ã  mesure
3. **PrÃ©-chargement** : Charger le modÃ¨le Ã  l'avance
4. **CDN** : Utiliser un CDN pour rÃ©duire la latence rÃ©seau

## âœ… Checklist de vÃ©rification

- [x] URL API correcte : `https://ai.jobsacademie.tech/api/chat`
- [x] ModÃ¨le correct : `gemma:2b`
- [x] Format de requÃªte Ollama implÃ©mentÃ©
- [x] Format de rÃ©ponse Ollama gÃ©rÃ©
- [x] Tests API rÃ©ussis
- [ ] Serveur dev redÃ©marrÃ©
- [ ] Tests dans l'interface web
- [ ] VÃ©rification console navigateur

## ğŸ“ Support

Si vous avez toujours des problÃ¨mes :

1. VÃ©rifiez que le serveur dev est redÃ©marrÃ©
2. Videz le cache du navigateur (Cmd+Shift+R sur Mac)
3. VÃ©rifiez les logs dans la console F12
4. Testez l'API directement avec curl

---

**Date de rÃ©solution** : 3 dÃ©cembre 2024  
**Status** : âœ… RÃ‰SOLU - API fonctionnelle  
**Prochaine Ã©tape** : RedÃ©marrer le serveur et tester !
